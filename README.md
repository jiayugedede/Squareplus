# Image-Algorithm-of-Paper
<!--This repository is sustained by Jiayu Zhang and leaded by Kunjie Chen professor, both in Engineering Institution of NJAU University in China.--><Br/>
This repository has two parts, the one is pattern recognition and the other is artificial intelligence, which will implement in python or other OOP codes. We want to devote ourself to promote agriclulture devolopment which include plant and food phenotypic. <Br/>
We will make great efforts to ensure and improve the quality of this repository which maybe the initial goal of this project.<Br/>

Pattern recognition:<Br/>
1. Image contrast<Br/>
   SHMS.py : Chang Y C , Chang C M . A simple histogram modification scheme for contrast enhancement[J]. IEEE Transactions on Consumer Electronics, 2010, 56(2):737-742.
2. Activation function:<Br/>
   (1) Jonathan T. Barron:Squareplus: A Softplus-Like Algebraic Rectifier. CoRR abs/2112.11687 (2021).// Implementing in tensorflow-2.5.0 .<Br/>
   (2) He K, Zhang X, Ren S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034.<Br/>
   (3) Zhao H, Liu F, Li L, et al. A novel softplus linear unit for deep convolutional neural networks[J]. Applied Intelligence, 2018, 48: 1707-1720.<Br/>
   (4) Biswas K, Kumar S, Banerjee S, et al. SMU: smooth activation function for deep networks using smoothing maximum technique[J]. arXiv preprint arXiv:2111.04682, 2021.<Br/>
   (5) Qiumei Z, Dan T, Fenghua W. Improved convolutional neural network based on fast exponentially linear unit activation function[J]. Ieee Access, 2019, 7: 151359-151367.<Br/>
   (6) Kili√ßarslan S, Celik M. RSigELU: A nonlinear activation function for deep neural networks[J]. Expert Systems with Applications, 2021, 174: 114805.<Br/>
   (7) Klambauer G, Unterthiner T, Mayr A, et al. Self-normalizing neural networks[J]. Advances in neural information processing systems, 2017, 30.<Br/>
   (8) Pratama K, Kang D K. Trainable activation function with differentiable negative side and adaptable rectified point[J]. Applied Intelligence, 2021, 51(3): 1784-1801.<Br/>
